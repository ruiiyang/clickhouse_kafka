{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate fake data for landing page, create a streaming data source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"photo\": {\"tags\": {\"tag\": [{\"id\": \"6e5f22b2-ff5b-43c4-9383-ef1ef8520b8a\", \"raw\": \"tag_9\"}, {\"id\": \"2e95c944-5e48-424e-bd08-c79a75654c63\", \"raw\": \"tag_39\"}]}}}\n",
      "{\"photo\": {\"tags\": {\"tag\": [{\"id\": \"693103e1-a612-4a21-b7c5-786f698f4686\", \"raw\": \"tag_62\"}, {\"id\": \"5312cb0b-9ab7-4de0-a194-bb298e4e6081\", \"raw\": \"tag_85\"}, {\"id\": \"ead001a9-608e-4283-b595-880ff3cd050b\", \"raw\": \"tag_67\"}, {\"id\": \"6e2c8e92-e853-49bb-8809-b7288ecd598e\", \"raw\": \"tag_47\"}]}}}\n",
      "{\"photo\": {\"tags\": {\"tag\": [{\"id\": \"09b63fc0-0eed-4567-a84a-65b676f9e7eb\", \"raw\": \"tag_76\"}, {\"id\": \"a918ae15-8000-4bfe-bead-207da0404036\", \"raw\": \"tag_35\"}]}}}\n",
      "{\"photo\": {\"tags\": {\"tag\": [{\"id\": \"7e21569c-7002-4ee3-9b68-4cab9040ce1b\", \"raw\": \"tag_13\"}]}}}\n",
      "{\"photo\": {\"tags\": {\"tag\": [{\"id\": \"ac4f093b-752c-4cff-bdc9-66e8dd095191\", \"raw\": \"tag_20\"}]}}}\n",
      "{\"photo\": {\"tags\": {\"tag\": [{\"id\": \"17be5423-9fc8-417a-a348-a13ce09da476\", \"raw\": \"tag_49\"}, {\"id\": \"ce4152a1-2439-4e2a-8012-e04f30f48480\", \"raw\": \"tag_57\"}]}}}\n",
      "{\"photo\": {\"tags\": {\"tag\": [{\"id\": \"9d5c584e-8ef9-4ddb-8c9c-0e0813e17599\", \"raw\": \"tag_75\"}, {\"id\": \"d7a3beb1-5d02-40cf-9b9d-05b0dc0fcd37\", \"raw\": \"tag_88\"}]}}}\n",
      "{\"photo\": {\"tags\": {\"tag\": [{\"id\": \"86562e55-4814-4702-803b-c036ae8c8fc7\", \"raw\": \"tag_78\"}, {\"id\": \"3be04783-f2f6-4a1f-8262-4660926d6881\", \"raw\": \"tag_17\"}]}}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m generate_fake_data_tags()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(fake_data)  \u001b[38;5;66;03m# In a real scenario, you would send this to an API or WebSocket\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for 1 second before sending the next fake data\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# this produce multiple tags id and raw of the photo \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Fixed template for the API data \n",
    "templat_json= {\n",
    "    \"photo\": {\n",
    "        \"tags\": {\n",
    "            \"tag\": [\n",
    "                {\n",
    "                    \"id\": \"string\", \n",
    "                    \"raw\": \"string\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to generate fake dynamic data\n",
    "def generate_fake_data_tags():\n",
    "    # Alter the 'id' and 'raw' fields dynamically\n",
    "    tags = []\n",
    "    for i in range(random.randint(1, 5)):  # Random number of tags per photo (1-5)\n",
    "        tags.append({\n",
    "            \"id\": str(uuid.uuid4()),  # Generate a unique ID using UUID\n",
    "            \"raw\": f\"tag_{random.randint(1, 100)}\"  # Random raw tag value\n",
    "        })\n",
    "\n",
    "    # Update the template with the fake tags\n",
    "    templat_json[\"photo\"][\"tags\"][\"tag\"] = tags\n",
    "    return json.dumps(templat_json)\n",
    "\n",
    "# Simulate streaming data every second\n",
    "while True:\n",
    "    fake_data = generate_fake_data_tags()\n",
    "    print(fake_data)  # In a real scenario, you would send this to an API or WebSocket\n",
    "    time.sleep(1)  # Wait for 1 second before sending the next fake data\n",
    "\n",
    "    # this produce multiple tags id and raw of the photo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: confluent-kafka in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.0)\n",
      "Building wheels for collected packages: avro-python3\n",
      "  Building wheel for avro-python3 (pyproject.toml): started\n",
      "  Building wheel for avro-python3 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43993 sha256=ad249ac12f06a3db4707e492c90dd856ff83aa7b5bca3e7ac6779ceaa6f943ec\n",
      "  Stored in directory: /Users/ruiiyoung/Library/Caches/pip/wheels/df/82/f4/b7126d86d6a404dd59a822fad5f169000deee5f61f7c88580c\n",
      "Successfully built avro-python3\n",
      "Installing collected packages: avro-python3\n",
      "Successfully installed avro-python3-1.10.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install avro-python3 confluent-kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Downloading https://github.com/ClickHouse/ClickHouse/releases/download/v25.2.1.3085-stable/clickhouse-macos\n",
      "==> Downloading from https://objects.githubusercontent.com/github-production-release-asset-2e65be/60246359/1f6c20d5-700c-41cc-bc39-f07ecc61eed6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250302%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250302T090400Z&X-Amz-Expires=300&X-Amz-Signature=7b5a769bdaf4de437df69c3b5d1e38f272f3aba9800123d7e24e487d27db1956&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dclickhouse-macos&response-content-type=application%2Foctet-stream\n",
      "==> Installing Cask clickhouse\n",
      "==> Linking Binary 'clickhouse-macos' to '/usr/local/bin/clickhouse'\n",
      "ðŸº  clickhouse was successfully installed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "brew install clickhouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clickhouse-driver\n",
      "  Downloading clickhouse_driver-0.2.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from clickhouse-driver) (2020.1)\n",
      "Collecting tzlocal (from clickhouse-driver)\n",
      "  Downloading tzlocal-5.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading clickhouse_driver-0.2.9-cp312-cp312-macosx_11_0_arm64.whl (217 kB)\n",
      "Downloading tzlocal-5.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: tzlocal, clickhouse-driver\n",
      "Successfully installed clickhouse-driver-0.2.9 tzlocal-5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install clickhouse-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent data: {'photo': {'tags': {'tag': [{'id': '316f0676-2635-4e59-b9a5-989330394166', 'raw': 'tag_81'}]}}}\n",
      "Sent data: {'photo': {'tags': {'tag': [{'id': '151fb3b6-882b-46f8-9a10-084ab44f7a47', 'raw': 'tag_96'}, {'id': '895e5e34-2a6e-4c40-9771-ec96475d2ed0', 'raw': 'tag_10'}, {'id': '09846712-8424-4f5d-b7f6-5a3a4a299e3f', 'raw': 'tag_76'}, {'id': 'a7fe64ce-688d-4de7-bbf7-c05b28552c30', 'raw': 'tag_31'}]}}}\n",
      "Sent data: {'photo': {'tags': {'tag': [{'id': 'e1c59215-27f9-444b-8b70-b7841e09d765', 'raw': 'tag_24'}, {'id': '28f8200c-ce5c-490d-b650-578d6ba1bfc2', 'raw': 'tag_80'}, {'id': '8a622824-b4e6-4192-8eec-e5aed7fe47e9', 'raw': 'tag_13'}]}}}\n",
      "Sent data: {'photo': {'tags': {'tag': [{'id': 'f12aab51-c7f0-44e2-b980-d0dfc2916b79', 'raw': 'tag_36'}, {'id': 'cb57d134-f40b-49e5-8fbd-e271af0ea8b4', 'raw': 'tag_6'}]}}}\n",
      "Sent data: {'photo': {'tags': {'tag': [{'id': 'a54f07a7-398a-48ce-b65f-a5be2ce16f49', 'raw': 'tag_56'}]}}}\n",
      "Sent data: {'photo': {'tags': {'tag': [{'id': '24f10ca0-dc83-4cc4-b69e-77d439e635c7', 'raw': 'tag_84'}, {'id': '39c8d089-8a49-41f4-9a7e-3444c9016eef', 'raw': 'tag_45'}, {'id': '2fd05c37-c49f-4cde-a1c5-29a36ffdbbb0', 'raw': 'tag_50'}]}}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m     produce_to_kafka(avro_data)  \u001b[38;5;66;03m# Send to Kafka\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSent data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# For debugging, you can also print the message\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for 1 second before generating the next batch of fake data\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import avro.schema\n",
    "import avro.io\n",
    "import json\n",
    "import uuid\n",
    "import random\n",
    "import time\n",
    "import io\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "# Define the Avro schema as before\n",
    "schema = avro.schema.Parse(\"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"TagData\",\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"photo\",\n",
    "      \"type\": {\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Photo\",\n",
    "        \"fields\": [\n",
    "          {\n",
    "            \"name\": \"tags\",\n",
    "            \"type\": {\n",
    "              \"type\": \"record\",\n",
    "              \"name\": \"Tags\",\n",
    "              \"fields\": [\n",
    "                {\n",
    "                  \"name\": \"tag\",\n",
    "                  \"type\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                      \"type\": \"record\",\n",
    "                      \"name\": \"Tag\",\n",
    "                      \"fields\": [\n",
    "                        {\"name\": \"id\", \"type\": \"string\"},\n",
    "                        {\"name\": \"raw\", \"type\": \"string\"}\n",
    "                      ]\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Function to generate fake dynamic data\n",
    "def generate_fake_data(num_messages=1):\n",
    "    all_photos = []\n",
    "    \n",
    "    for _ in range(num_messages):  # Generate the specified number of messages\n",
    "        tags = []\n",
    "        \n",
    "        # Random number of tags per photo (1-5 tags)\n",
    "        for i in range(random.randint(1, 5)):  \n",
    "            tags.append({\n",
    "                \"id\": str(uuid.uuid4()),  # Generate a unique ID using UUID\n",
    "                \"raw\": f\"tag_{random.randint(1, 100)}\"  # Random raw tag value\n",
    "            })\n",
    "        \n",
    "        # Initialize template JSON (This is the structure you're sending to Kafka)\n",
    "        templat_json = {\n",
    "            \"photo\": {\n",
    "                \"tags\": {\n",
    "                    \"tag\": tags\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Append the generated photo to the list of all photos\n",
    "        all_photos.append(templat_json)\n",
    "    \n",
    "    return all_photos  # Return the list of photo messages\n",
    "\n",
    "# Function to serialize data to Avro format\n",
    "def serialize_avro(data, schema):\n",
    "    buf = io.BytesIO()\n",
    "    writer = avro.io.DatumWriter(schema)\n",
    "    encoder = avro.io.BinaryEncoder(buf)\n",
    "    writer.write(data, encoder)\n",
    "    return buf.getvalue()\n",
    "\n",
    "# Initialize Kafka producer\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Replace with your Kafka address\n",
    "    'client.id': 'python-producer'\n",
    "}\n",
    "\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Send data to Kafka\n",
    "def produce_to_kafka(avro_data):\n",
    "    producer.produce('photo.tag.bdr.events', avro_data)  # Replace with your Kafka topic name\n",
    "    producer.flush()\n",
    "\n",
    "# Simulate streaming data every second\n",
    "while True:\n",
    "    num_messages = random.randint(1, 5)  # Random number of messages per second (1-5 messages)\n",
    "    fake_data = generate_fake_data(num_messages=num_messages)  # Generate fake data\n",
    "    \n",
    "    for message in fake_data:\n",
    "        avro_data = serialize_avro(message, schema)  # Convert to Avro format\n",
    "        produce_to_kafka(avro_data)  # Send to Kafka\n",
    "        print(f\"Sent data: {message}\")  # For debugging, you can also print the message\n",
    "    \n",
    "    time.sleep(6)  # Wait for 1 second before generating the next batch of fake data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending data to Kafka: {\"photo\": {\"tags\": {\"tag\": [{\"id\": \"bf9da861-fe67-48ab-9b19-7e8b86853377\", \"raw\": \"tag_1\"}, {\"id\": \"aaa05b73-1ff1-41c8-ae36-c676161e02d1\", \"raw\": \"tag_82\"}, {\"id\": \"f2d52f1a-bab0-4656-b802-82192e97c9f7\", \"raw\": \"tag_14\"}]}}}\n",
      "Message delivered to photo.tag.bdr.events [2]\n",
      "Sending data to Kafka: {\"photo\": {\"tags\": {\"tag\": [{\"id\": \"1bf51e1e-2da6-4eb8-a542-1d2ec6af4e98\", \"raw\": \"tag_72\"}, {\"id\": \"d6192fa5-d14f-4a2b-89c9-1c86d3cff9f8\", \"raw\": \"tag_32\"}, {\"id\": \"f52e2b78-aee3-4979-a13c-77c8f8e46ec2\", \"raw\": \"tag_70\"}, {\"id\": \"881e36af-d970-4d94-b03e-5f55eb73da17\", \"raw\": \"tag_84\"}]}}}\n",
      "Message delivered to photo.tag.bdr.events [1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending data to Kafka: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfake_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# For debugging, you can see the data being sent\u001b[39;00m\n\u001b[1;32m     57\u001b[0m produce_to_kafka(fake_data)  \u001b[38;5;66;03m# Send to Kafka\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for 1 second before sending the next fake data\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "# Kafka Config\n",
    "KAFKA_BROKER = \"localhost:9092\"\n",
    "KAFKA_TOPIC = \"photo.tag.bdr.events\"\n",
    "\n",
    "# Configure Kafka producer\n",
    "producer_conf = {\n",
    "    \"bootstrap.servers\": KAFKA_BROKER,\n",
    "    \"client.id\": \"python-producer\"\n",
    "}\n",
    "\n",
    "producer = Producer(producer_conf)\n",
    "\n",
    "# Function to generate fake dynamic data\n",
    "def generate_fake_data_tags():\n",
    "\n",
    "\n",
    "    tags = []\n",
    "    for i in range(random.randint(1, 5)):  # Random number of tags per photo (1-5)\n",
    "        tags.append({\n",
    "            \"id\": str(uuid.uuid4()),  # Generate a unique ID using UUID\n",
    "            \"raw\": f\"tag_{random.randint(1, 100)}\"  # Random raw tag value\n",
    "        })\n",
    "\n",
    "    # Construct the template with fake tags\n",
    "    template_json = {\n",
    "        \"photo\": {\n",
    "            \"tags\": {\n",
    "                \"tag\": tags\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return json.dumps(template_json)\n",
    "\n",
    "# Function to send data to Kafka with delivery callback\n",
    "def produce_to_kafka(data):\n",
    "    def delivery_report(err, msg):\n",
    "        if err is not None:\n",
    "            print(f\"Message delivery failed: {err}\")\n",
    "        else:\n",
    "            print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
    "\n",
    "    try:\n",
    "        producer.produce(KAFKA_TOPIC, value=data.encode('utf-8'), callback=delivery_report)  # Ensure the data is in UTF-8 format\n",
    "        producer.flush()  # Ensure the message is sent immediately\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending data to Kafka: {e}\")\n",
    "\n",
    "# Simulate streaming data every second\n",
    "while True:\n",
    "    fake_data = generate_fake_data_tags()\n",
    "    print(f\"Sending data to Kafka: {fake_data}\")  # For debugging, you can see the data being sent\n",
    "    produce_to_kafka(fake_data)  # Send to Kafka\n",
    "    time.sleep(5)  # Wait for 1 second before sending the next fake data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
